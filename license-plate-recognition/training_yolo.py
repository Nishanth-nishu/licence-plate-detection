# -*- coding: utf-8 -*-
"""yolo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12kzASLPNCVGzVdUx7rEgqIXt40zfN8uu
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive',force_remount=True)

# Install required libraries
!pip install torch torchvision --quiet
!pip install easyocr --quiet
!pip install opencv-python-headless --quiet
!pip install ultralytics --quiet  # For YOLOv8

import os
import cv2
import numpy as np
import torch
from torchvision import transforms
from PIL import Image
import easyocr
from ultralytics import YOLO

import zipfile
import os
!pip install torch torchvision --quiet
!pip install easyocr --quiet
!pip install opencv-python-headless --quiet
!pip install ultralytics --quiet  # For YOLOv8

import os
import cv2
import numpy as np
import torch
from torchvision import transforms
from PIL import Image
import easyocr
from ultralytics import YOLO

# Paths to the zip files
detection_zip_path = '/content/drive/MyDrive/DATA SCIENTIST_ASSIGNMENT/test.zip'  # The zip file for detection images
recognition_zip_path = '/content/drive/MyDrive/DATA SCIENTIST_ASSIGNMENT/Licplatesrecognition_train.zip'  # The zip file for recognition images

# Destination folders where the images will be extracted
detection_extract_path = '/content/extracted_detection_images'
recognition_extract_path = '/content/extracted_recognition_images'

# Function to extract zip files
def extract_zip(zip_path, extract_to):
    if not os.path.exists(extract_to):
        os.makedirs(extract_to)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print(f"Extracted {zip_path} to {extract_to}")

# Extract detection and recognition images
extract_zip(detection_zip_path, detection_extract_path)
extract_zip(recognition_zip_path, recognition_extract_path)

df.columns

import os
import pandas as pd

# Paths
CSV_FILE_PATH = "/content/drive/MyDrive/DATA SCIENTIST_ASSIGNMENT/Licplatesdetection_train.csv"  # Path to the CSV file
IMAGE_DIR = "/content/extracted_detection_images/license_plates_detection_train"  # Path to the folder containing images

# Load CSV file
try:
    df = pd.read_csv(CSV_FILE_PATH, delimiter=",")  # Assuming tab-delimited CSV
    print(f"Loaded {len(df)} annotations from {CSV_FILE_PATH}.")
except Exception as e:
    print(f"Error loading CSV: {e}")
    raise

# Extract unique image IDs from the CSV
csv_image_ids = set(df['img_id'].unique())
print(f"Unique image IDs in CSV: {len(csv_image_ids)}")

# Get all image filenames in the directory
image_filenames = set(os.listdir(IMAGE_DIR))
print(f"Total images in directory: {len(image_filenames)}")

# Cross-check
images_with_annotations = csv_image_ids & image_filenames
missing_annotations = image_filenames - csv_image_ids
orphan_annotations = csv_image_ids - image_filenames

# Results
print("\nSummary:")
print(f"Images with annotations: {len(images_with_annotations)}")
print(f"Images without annotations: {len(missing_annotations)}")
print(f"Annotations without images: {len(orphan_annotations)}")

# Save results for review
with open("missing_annotations.txt", "w") as f:
    f.writelines(f"{img}\n" for img in missing_annotations)

with open("orphan_annotations.txt", "w") as f:
    f.writelines(f"{img}\n" for img in orphan_annotations)

print("\nDetails saved to 'missing_annotations.txt' and 'orphan_annotations.txt'.")

import os
import pandas as pd
from PIL import Image

# Paths
csv_file_path = "/content/drive/MyDrive/DATA SCIENTIST_ASSIGNMENT/Licplatesdetection_train.csv"
images_directory = "/content/extracted_detection_images/license_plates_detection_train"
output_directory = "/content/yolo_labels"

# Create output directory if it doesn't exist
os.makedirs(output_directory, exist_ok=True)

# Load the CSV file
data = pd.read_csv(csv_file_path)
data.columns = [col.strip() for col in data.columns]  # Clean column names

# Ensure the CSV has the correct structure
required_columns = {"img_id", "xmin", "ymin", "xmax", "ymax"}
if not required_columns.issubset(data.columns):
    raise ValueError(f"CSV file must contain the following columns: {required_columns}")

# Iterate through each image and process annotations
for img_id, group in data.groupby("img_id"):
    image_path = os.path.join(images_directory, img_id)
    if not os.path.exists(image_path):
        print(f"Warning: Image {img_id} not found in {images_directory}. Skipping.")
        continue

    # Open the image to get dimensions
    with Image.open(image_path) as img:
        img_width, img_height = img.size

    # Prepare YOLO format annotations
    yolo_annotations = []
    for _, row in group.iterrows():
        # Normalize coordinates
        x_center = ((row["xmin"] + row["xmax"]) / 2) / img_width
        y_center = ((row["ymin"] + row["ymax"]) / 2) / img_height
        width = (row["xmax"] - row["xmin"]) / img_width
        height = (row["ymax"] - row["ymin"]) / img_height

        # Append class ID (0 for single class) and normalized coordinates
        yolo_annotations.append(f"0 {x_center} {y_center} {width} {height}")

    # Save annotations to a text file in YOLO format
    output_file_path = os.path.join(output_directory, os.path.splitext(img_id)[0] + ".txt")
    with open(output_file_path, "w") as f:
        f.write("\n".join(yolo_annotations))

print(f"YOLO annotations saved to: {output_directory}")

import os
import shutil
import random

# Paths
images_directory = '/content/extracted_detection_images/license_plates_detection_train'  # Path to images
labels_directory = '/content/yolo_labels'  # Path to YOLO annotations
train_images_dir = '/content/train/images'  # Path for training images
train_labels_dir = '/content/train/labels'  # Path for training labels
val_images_dir = '/content/val/images'  # Path for validation images
val_labels_dir = '/content/val/labels'  # Path for validation labels

# Create train and val directories if they don't exist
os.makedirs(train_images_dir, exist_ok=True)
os.makedirs(train_labels_dir, exist_ok=True)
os.makedirs(val_images_dir, exist_ok=True)
os.makedirs(val_labels_dir, exist_ok=True)

# Get the list of image files
image_files = [f for f in os.listdir(images_directory) if f.endswith('.jpg')]  # Assuming images are in .jpg format

# Shuffle the list of image files
random.shuffle(image_files)

# Split into 80% train and 20% validation
split_ratio = 0.8
train_size = int(len(image_files) * split_ratio)
train_files = image_files[:train_size]
val_files = image_files[train_size:]

# Function to move files to the appropriate directories
def move_files(files, src_image_dir, src_label_dir, dest_image_dir, dest_label_dir):
    for file in files:
        # Move images
        shutil.move(os.path.join(src_image_dir, file), os.path.join(dest_image_dir, file))

        # Move corresponding label files
        label_file = os.path.splitext(file)[0] + '.txt'  # Assuming labels have the same name as images
        shutil.move(os.path.join(src_label_dir, label_file), os.path.join(dest_label_dir, label_file))

# Move training and validation files
move_files(train_files, images_directory, labels_directory, train_images_dir, train_labels_dir)
move_files(val_files, images_directory, labels_directory, val_images_dir, val_labels_dir)

print(f"Dataset split into {len(train_files)} training and {len(val_files)} validation samples.")

import yaml

# Define paths for training and validation sets
train_images_path = '/content/train/images'
val_images_path = '/content/val/images'

# Number of classes
num_classes = 1
class_names = ['license_plate']

# Create the data.yaml content
data_yaml = {
    'train': train_images_path,
    'val': val_images_path,
    'nc': num_classes,
    'names': class_names
}

# Define the path for saving the data.yaml file
yaml_file_path = '/content/data.yaml'

# Write the YAML file
with open(yaml_file_path, 'w') as yaml_file:
    yaml.dump(data_yaml, yaml_file)

print(f"data.yaml file has been created and saved to {yaml_file_path}")

from ultralytics import YOLO

# Path to your custom data.yaml file
data_yaml_path = '/content/data.yaml'

# Load the YOLOv8 model (can be yolov8n, yolov8s, yolov8m, yolov8l, etc. depending on your requirements)
model = YOLO('yolov8n.yaml')  # You can try 'yolov8s.yaml' or 'yolov8m.yaml' for better performance

# Start training with augmentations enabled and training on GPU
results = model.train(
    data=data_yaml_path,          # Path to the data.yaml
    epochs=50,                    # Number of epochs (adjust based on your needs)
    batch=16,                     # Correct parameter name for batch size
    imgsz=640,                    # Correct parameter name for image size
    project='/content/yolo_training',  # Directory to save the training outputs
    name='license_plate_model',   # Subfolder name to store this training run
    save=True,                    # Save the model and outputs
    exist_ok=True,                # Overwrite existing folder if necessary
    device=0,                     # Train on the first GPU (set to 0 for a single GPU)
    augment=True                  # Enable data augmentation (this is the default behavior)
)

# Check training results and save model weights, logs, and metrics

results = model.val()
print(results)

results = model.predict("/content/train/images/26.jpg")
print(results)

import torch
import cv2
import matplotlib.pyplot as plt

# Load a test image
image = cv2.imread('/content/extracted_detection_images/license_plates_detection_train/130.jpg')


results = model(image)

# Check if results are returned as a list and access the first image's results
if isinstance(results, list):
    results = results[0]  # Get the first result from the list (in case of multiple images)

# The 'results' object should now have attributes like 'xyxy', 'confidence', etc.
# To print detection results (e.g., bounding boxes and confidence scores)
print("Detected boxes and confidence scores:")
for box in results.boxes:
    # Get bounding box coordinates (x1, y1, x2, y2)
    x1, y1, x2, y2 = box.xyxy[0].tolist()
    conf = box.conf[0].item()  # Confidence score
    cls = box.cls[0].item()  # Class ID (e.g., 'license_plate' ID)

    print(f"License plate detected at coordinates: ({x1}, {y1}), ({x2}, {y2}) with confidence: {conf:.2f} and class ID: {cls}")

    # Draw bounding boxes on the image
    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
    cv2.putText(image, f"{conf:.2f}", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

# Show the image with bounding boxes
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.show()

model.save('best.pt')

from ultralytics import YOLO

# Path to the trained model weights
model_path = '/content/best (1).pt'

# Load the trained model
model = YOLO(model_path)

!pip install pytesseract

import cv2
import pandas as pd
from pathlib import Path
import easyocr
from ultralytics import YOLO  # Use the ultralytics YOLO model

# Initialize EasyOCR Reader (English language)
reader = easyocr.Reader(['en'])

# Path to the trained model weights
model_path = '/content/best (1).pt'

# Load the trained model
model = YOLO(model_path)  # Load the custom YOLOv5 model

# Function to extract text using EasyOCR
def extract_text_from_plate(cropped_img):
    # Perform OCR using EasyOCR
    results = reader.readtext(cropped_img)
    extracted_text = ""

    # Collect text from the results (assuming the largest detected text block is the license plate)
    for result in results:
        extracted_text += result[1]  # Extract the text part

    return extracted_text.strip()

# Function to process and extract text from license plates
def process_license_plates(image_path):
    img = cv2.imread(image_path)
    results = model(image_path)  # Use YOLO to detect objects in the image

    # Iterate through the results (each is a detection)
    license_plates = []
    for result in results[0].boxes:  # Access the first item in the list (single image)
        # Each result contains bounding box coordinates, class, and confidence
        xmin, ymin, xmax, ymax = result.xyxy[0]  # Extract bounding box coordinates
        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)

        # Check if the detected object is a license plate (class 0 for license plates)
        if result.cls[0] == 0:  # Class 0 corresponds to license plates in your model
            cropped_img = img[ymin:ymax, xmin:xmax]  # Crop the detected license plate

            # Perform text extraction using EasyOCR
            plate_text = extract_text_from_plate(cropped_img)

            # Add extracted text to list
            license_plates.append(plate_text)

    return license_plates

# Set the path to your test images directory
test_images_path = Path("/content/extracted_detection_images/test/test")  # Replace with the correct directory path

# Prepare CSV data
csv_data = []

# Character-to-index mapping (assuming you already have this mapping)
char_to_idx = {str(i): i for i in range(10)}  # Example mapping (modify it based on your model)

# Iterate over all test images
for img_path in test_images_path.glob("*.jpg"):
    license_plates = process_license_plates(str(img_path))  # Extract license plate text
    for i, plate_text in enumerate(license_plates):
        # Ensure plate_text is no longer than 10 characters, truncate if necessary
        plate_text = plate_text[:10]  # Truncate to 10 characters if it's too long

        # Create a row with the image id and the characters in the license plate
        row = [f"{img_path.stem}_{i+1}"] + [char_to_idx.get(c, 0) for c in plate_text]

        # Make sure the row has a length of 11 (1 for id + 10 for characters)
        row.extend([0] * (11 - len(row)))  # Ensure exactly 10 characters per row

        # Add the row to csv data
        csv_data.append(row)

# Save the results to a CSV
df = pd.DataFrame(csv_data, columns=["id"] + [str(i) for i in range(10)])
df.to_csv("license_plate_predictions2.csv", index=False)
#
print("CSV file with license plate predictions has been saved successfully.")
